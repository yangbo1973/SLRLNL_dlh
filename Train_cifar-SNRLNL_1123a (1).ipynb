{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1fac75f-1781-44c2-94e9-e952634a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from Models import *\n",
    "from torchvision.models import vgg19_bn\n",
    "\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import data_loader_cifar as dataloader\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "#40 asym, 0.04 xi, eta 10, nc 20, nv 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f974b76-a9eb-4da5-b565-a288657e66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR Training')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='train batchsize') \n",
    "parser.add_argument('--lr', '--learning_rate', default=0.02, type=float, help='initial learning rate')\n",
    "parser.add_argument('--noise_mode',  default='sym_0.5', help = 'aggre,worst,rand1,rand2,rand3,noisy100, or sym_0.x, asym_0.x')\n",
    "parser.add_argument('--num_epochs', default=300, type=int)\n",
    "parser.add_argument('--t_w', default=10, type=int)\n",
    "parser.add_argument('--nR', default=0.04, type=float)\n",
    "parser.add_argument('--nc', default=0.2, type=float)\n",
    "parser.add_argument('--nv', default=0.8, type=float)\n",
    "parser.add_argument('--nvs', default=0.1, type=float)\n",
    "parser.add_argument('--id', default='')\n",
    "parser.add_argument('--seed', default=123)\n",
    "parser.add_argument('--gpuid', default=0, type=int)\n",
    "parser.add_argument('--data_path', default='./cifar-10', type=str, help='path to dataset')\n",
    "parser.add_argument('--dataset', default='cifar10', type=str)\n",
    "args = parser.parse_args(args = ['--data_path', 'data/CIFAR10',\n",
    "                                 '--dataset', 'cifar10',\n",
    "                                 '--noise_mode','asym_0.4',\n",
    "                                 '--t_w', '10',\n",
    "                                 '--batch_size','64',\n",
    "                                 '--lr','0.02',\n",
    "                                 '--num_epochs','150',\n",
    "                                 '--nR', '0.1',\n",
    "                                 '--nc','0.1',\n",
    "                                 '--nv','0.2',\n",
    "                                 '--nvs','0.02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645caeb7-89ec-45f0-860b-e3395c75bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpuid)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c17f27b-2eff-425b-a66b-867171724336",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 50000\n",
    "test_samples = 10000\n",
    "if args.dataset == 'cifar10':\n",
    "    n_class = 10\n",
    "else:\n",
    "    n_class = 100\n",
    "feature_num = 512\n",
    "t_w = args.t_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decf99d1-a605-4b67-a2ef-0f64985f472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,net,):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    feature_temp = np.zeros((test_samples, feature_num))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, ind) in enumerate(test_loader):\n",
    "            ind = ind.numpy()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            feature, output = net(inputs)       \n",
    "            _, predicted = torch.max(output, 1)     \n",
    "            \n",
    "            feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "                       \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).cpu().sum().item()                 \n",
    "    acc = 100.*correct/total\n",
    "    \n",
    "    test_log.write('Epoch:%d   Accuracy:%.2f\\n'%(epoch,acc))\n",
    "    test_log.flush()  \n",
    "    \n",
    "    lossb = relevant_hard_np(feature_temp)\n",
    "    return acc, lossb, feature_temp\n",
    "\n",
    "\n",
    "def linear_rampup(current, warm_up, rampup_length=16):\n",
    "    current = np.clip((current-warm_up) / rampup_length, 0.0, 1.0)\n",
    "    return args.lambda_u*float(current)\n",
    "\n",
    "\n",
    "class NegEntropy(object):\n",
    "    def __call__(self,outputs):\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        return torch.mean(torch.sum(probs.log()*probs, dim=1))\n",
    "    \n",
    "\n",
    "class Orthogonal_loss(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Orthogonal_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, x, ):\n",
    "        n = x.size(0)\n",
    "        m = x.size(1)\n",
    "\n",
    "        I = torch.eye(m).cuda()\n",
    "        e = x - x.mean(dim=0, keepdims = True)\n",
    "        m_nonz = (e.sum(dim = 0) != 0).sum()\n",
    "        \n",
    "        cov = e.T @ e\n",
    "        \n",
    "        cov2 = cov ** 2\n",
    "        \n",
    "        select_i = torch.argmax(cov2 - cov2 * I, dim = 1)\n",
    "        cov_m = (F.one_hot(select_i, num_classes = m) * cov2).sum()\n",
    "        cov_i = (I * cov).sum()\n",
    "        \n",
    "        result = (cov_m-cov_i) / (m_nonz * n)\n",
    "        return result\n",
    "    \n",
    "def relevant_hard_np(x,):\n",
    "    n = x.shape[0]\n",
    "    nz = x.shape[1]\n",
    "    e = x - x.mean(axis = 0,keepdims = True)\n",
    "\n",
    "    cov = e.T @ e\n",
    "\n",
    "    sigma = (e ** 2).sum(axis = 0, keepdims = True)\n",
    "    r = cov / (sigma.T @ sigma) ** 0.5\n",
    "\n",
    "    r = r ** 2\n",
    "    r[np.isnan(r)] = 0.0\n",
    "\n",
    "    return np.mean(np.max(r - r * np.eye(nz), axis = -1))\n",
    "    \n",
    "class MSELoss(object):\n",
    "    def __call__(self, logits, targets,):\n",
    "        if len(targets.shape) == 1:\n",
    "            targets = F.one_hot(targets, num_classes=n_class)\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        Lu = torch.mean((probs - targets)**2, dim = -1)\n",
    "\n",
    "        return Lu\n",
    "    \n",
    "def create_model():\n",
    "    model = ResNet18(num_classes=n_class)\n",
    "    model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd574e5e-b996-4f8a-ab16-86abf8881e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_log=open('./checkpoint/SNRLNL_%s_%s_%s'%(\n",
    "    args.dataset,args.noise_mode,str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_stats.txt','w') \n",
    "test_log=open('./checkpoint/SNRLNL_%s_%s_%s'%(\n",
    "    args.dataset,args.noise_mode,str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_acc.txt','w')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1ed60c-1164-4581-bcc6-faa8d0ed8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_noise_types = ['aggre','worst','rand1','rand2','rand3','noisy100']\n",
    "synthetic_noise_types = ['sym','asym']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09d8544-6786-479b-b3e0-02a03d76007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Building net\n"
     ]
    }
   ],
   "source": [
    "if args.noise_mode in real_world_noise_types:\n",
    "    if args.dataset == 'cifar10':\n",
    "        loader = dataloader.cifar_dataloader(args.dataset,noise_mode=args.noise_mode,batch_size=args.batch_size,num_workers=5,\\\n",
    "            root_dir=args.data_path,log=stats_log,noise_file='%s/CIFAR-10_human.pt'%(args.data_path,))\n",
    "    else:\n",
    "        loader = dataloader.cifar_dataloader(args.dataset,noise_mode=args.noise_mode,batch_size=args.batch_size,num_workers=5,\\\n",
    "            root_dir=args.data_path,log=stats_log,noise_file='%s/CIFAR-100_human.pt'%(args.data_path,))\n",
    "else:\n",
    "    loader = dataloader.cifar_dataloader(args.dataset,noise_mode=args.noise_mode,batch_size=args.batch_size,num_workers=5,\\\n",
    "        root_dir=args.data_path,log=stats_log,noise_file='%s/%s.json'%(args.data_path,args.noise_mode))\n",
    "\n",
    "print('| Building net')\n",
    "net = create_model()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "opt = optim.SGD(net.parameters(),\n",
    "                lr=args.lr,\n",
    "                momentum=0.9,\n",
    "                weight_decay=5e-4\n",
    "               )\n",
    "\n",
    "sch = optim.lr_scheduler.MultiStepLR(opt, [50, 100,], gamma = 0.1)\n",
    "\n",
    "CE = nn.CrossEntropyLoss(reduction='none')\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "MSE = MSELoss()\n",
    "\n",
    "loss_ortho = Orthogonal_loss()\n",
    "\n",
    "all_loss = [[],[]] # save the history of losses from two networks\n",
    "\n",
    "traindataset, trainloader = loader.run('warmup')\n",
    "testdataset, test_loader = loader.run('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff8aa9-0093-48c2-b689-08a676c52052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_Y = np.array(traindataset.train_label)\n",
    "test_Y = np.array(testdataset.test_label)\n",
    "noisy_Y = np.array(traindataset.noise_label)\n",
    "revised_Y = np.array(traindataset.noise_label)\n",
    "revised_Y_before = np.array(traindataset.noise_label)\n",
    "\n",
    "Yt_list = [np.array(traindataset.noise_label)]\n",
    "acc_list = []\n",
    "loss_sep_list = [[]]\n",
    "loss_train_list = []\n",
    "Py_temp_list = []\n",
    "\n",
    "score = np.random.rand(samples,)\n",
    "score_uncertainty = np.random.rand(samples,)\n",
    "\n",
    "OOD_mask_before = np.zeros((samples,),np.bool)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "    feature_num = 512\n",
    "    if epoch < t_w:\n",
    "        _, trainloader = loader.run('warmup')\n",
    "    else:\n",
    "        _, trainloader = loader.run('train')\n",
    "\n",
    "    loss_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Pred_other_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    \n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    "\n",
    "    if len(Py_temp_list) > 1:\n",
    "        score = Py_temp_list[-1]\n",
    "    else:\n",
    "        score = np.random.rand(samples,)\n",
    "        \n",
    "    OOD_mask = np.logical_and(score < np.sort(score[~OOD_mask_before])[int(len(score[~OOD_mask_before]) * args.nR)], ~OOD_mask_before)\n",
    "    \n",
    "    Y_onehot = np.eye(n_class)[revised_Y].astype(np.float32)\n",
    "    Y_onehot_0 = np.eye(n_class)[noisy_Y].astype(np.float32)\n",
    "    \n",
    "    for batch_id, (X_data, targets, ind) in enumerate(trainloader):\n",
    "        ind = ind.numpy()\n",
    "\n",
    "        Y_data = np.array(revised_Y[ind]).astype(np.int64)\n",
    "        Y_data_ori = np.array(train_Y[ind]).astype(np.int64)\n",
    "        Y_data_before = np.array(revised_Y_before[ind]).astype(np.int64)\n",
    "        temp_X = X_data.cuda()\n",
    "        opt.zero_grad()\n",
    "        Y_GPU = torch.from_numpy(Y_data).cuda()\n",
    "        Y_GPU_ori = torch.from_numpy(Y_data_ori).cuda()\n",
    "        Y_GPU_before = torch.from_numpy(Y_data_before).cuda()\n",
    "        \n",
    "        y_onehot = F.one_hot(Y_GPU.view(-1,),num_classes=n_class)\n",
    "        y_onehot_before = F.one_hot(Y_GPU_before.view(-1,),num_classes=n_class)\n",
    "        \n",
    "        feature, logits = net(temp_X)\n",
    "\n",
    "        probs = logits.softmax(1)\n",
    "        Py = torch.sum(y_onehot * probs, dim = -1)\n",
    "        Pred = probs.argmax(-1)\n",
    "        logits_other = logits - logits * y_onehot_before\n",
    "        Pred_other = torch.argmax(logits_other,dim=-1)\n",
    "        \n",
    "        if epoch < args.t_w:\n",
    "            loss = CEloss(logits,Y_GPU.view(-1,))\n",
    "        else:\n",
    "            Y_GPU = torch.where(torch.from_numpy(OOD_mask[ind]).cuda(), Pred_other, Y_GPU)\n",
    "            loss = CEloss(logits,Y_GPU.view(-1,))\n",
    "            \n",
    "        Py_temp[ind] = Py.cpu().detach().numpy()\n",
    "        Pred_temp[ind] = Pred.cpu().detach().numpy()\n",
    "\n",
    "        Probs_temp[ind] = probs.cpu().detach().numpy()\n",
    "        Logits_temp[ind] = logits.cpu().detach().numpy()\n",
    "        feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "        \n",
    "        Pred_other_temp[ind] = Pred_other.cpu().detach().numpy()\n",
    "                \n",
    "        correct = (Pred == Y_GPU).sum().item()\n",
    "        correct_ori = (Pred == Y_GPU_ori).sum().item()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "        acc_train += correct\n",
    "        acc_train_ori += correct_ori\n",
    "        \n",
    "        loss_sep_list[-1].append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "    sch.step()\n",
    "    \n",
    "    loss_train/=(batch_id+1)\n",
    "    acc_train/=samples\n",
    "    acc_train_ori/=samples\n",
    "\n",
    "    print('epoch %d train complete'%epoch)\n",
    "    acc_eval, lossb_eval, feature_val = test(epoch, net)\n",
    "    \n",
    "    loss_b = relevant_hard_np(feature_temp[:10000])\n",
    "\n",
    "    acc_list.append(acc_eval)\n",
    "    Py_temp_list.append(Py_temp)\n",
    "    \n",
    "    revised_Y_before = revised_Y.copy()\n",
    "    OOD_mask_before = OOD_mask.copy()    \n",
    "        \n",
    "    if epoch < t_w:\n",
    "        select = np.zeros((samples,),dtype = np.bool)\n",
    "        score = np.random.rand(samples,)\n",
    "    else:\n",
    "        nC_points = []\n",
    "        clean_mask = np.zeros((samples,),dtype=bool)\n",
    "        Py_mean = np.zeros((samples,))\n",
    "        for j in range(len(Py_temp_list)):\n",
    "            Py_mean+=Py_temp_list[j]\n",
    "        Py_mean/=len(Py_temp_list)   \n",
    "        for j in range(n_class):\n",
    "            class_mask = noisy_Y == j\n",
    "            c_n = class_mask.sum()\n",
    "            c_th = np.sort(Py_mean[class_mask])[-int(c_n * args.nc)]\n",
    "            nC_points.append(np.where(np.logical_and(Py_mean>=c_th,class_mask))[0])\n",
    "        nC_points = np.concatenate(nC_points)\n",
    "        L_batch = 1000\n",
    "        Y_onehot = np.eye(n_class)[revised_Y]\n",
    "        zC = torch.from_numpy(feature_temp[nC_points].astype(np.float32)).cuda()\n",
    "        fC = torch.from_numpy(Probs_temp[nC_points].astype(np.float32)).cuda()\n",
    "        yC = torch.from_numpy(Y_onehot[nC_points].astype(np.float32)).cuda()\n",
    "        fCcyC = fC - yC\n",
    "        lr = 1e-6\n",
    "        learning_risk = np.zeros((samples,)) \n",
    "        for j in range(int(np.ceil(samples/L_batch))):\n",
    "            i_ind = np.arange(j*L_batch, min(samples,(j+1)*L_batch))\n",
    "            zi = torch.from_numpy(feature_temp[i_ind].astype(np.float32)).cuda()\n",
    "            fi = torch.from_numpy(Probs_temp[i_ind].astype(np.float32)).cuda()\n",
    "            yi = torch.from_numpy(Y_onehot[i_ind].astype(np.float32)).cuda()\n",
    "            zixzC = zi @ zC.transpose(1,0)\n",
    "            part_1_1 = (zixzC + 1) @ fCcyC\n",
    "            part1 = (part_1_1 * (yi-fi)).sum(dim=-1,keepdim=True)*4*lr/len(nC_points)\n",
    "            part1_all = part_1_1 * (fi-torch.ones_like(yi))*4*lr/len(nC_points)\n",
    "\n",
    "            learning_risk[i_ind] = part1.cpu().detach().numpy().ravel()\n",
    "        # r_ = min(0.1 + 0.1 * (epoch - t_w), args.nv)\n",
    "        r_ = min(args.nvs + args.nvs * (epoch - t_w), args.nv)\n",
    "        # select = np.zeros((samples,),dtype=np.bool8)\n",
    "        # for j in range(n_class):\n",
    "        #     class_mask = revised_Y == j\n",
    "        #     c_n = class_mask.sum()\n",
    "        #     c_th = np.sort(learning_risk[class_mask])[-min(int(c_n * r_),c_n)]\n",
    "        #     select[np.logical_and(learning_risk>=c_th, class_mask)] = True\n",
    "        th = np.sort(learning_risk)[-min(int(samples * r_),samples)]\n",
    "        select = learning_risk >= th\n",
    "        \n",
    "        revised_Y=np.where(select.ravel(), Pred_temp.ravel(), noisy_Y.ravel()).astype(int)\n",
    "\n",
    "    is_noise = revised_Y != train_Y\n",
    "    max_noised_class = -999\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = train_Y == j_\n",
    "        noise_n = np.logical_and(class_mask, is_noise).sum()\n",
    "        if noise_n > max_noised_class:\n",
    "            max_noised_class = noise_n\n",
    "    \n",
    "\n",
    "    Yt_remain_noise = np.sum(is_noise)\n",
    "    end_time = time.time()\n",
    "    print_str = 'loss_b:%.4f, loss_b_eval:%.4f, train loss:%.4f, train acc:%.4f, train acc ori:%.4f,\\\n",
    "          eval acc:%.4f, time elapsed:%.4f, epoch %d train cleaned, %d samples changed,\\\n",
    "          total remain noise:%.4d, max class noise:%d'%(loss_b,\n",
    "                                                        lossb_eval,\n",
    "                                                        loss_train,\n",
    "                                                        acc_train,\n",
    "                                                        acc_train_ori, \n",
    "                                                        acc_eval,\n",
    "                                                        end_time - start_time,\n",
    "                                                        epoch,\n",
    "                                                        np.sum(revised_Y!=noisy_Y),\n",
    "                                                        Yt_remain_noise,\n",
    "                                                        max_noised_class)\n",
    "    print(print_str)\n",
    "    stats_log.write(print_str+'\\n')\n",
    "    stats_log.flush()  \n",
    "    # loss_sep_list.append([])\n",
    "    Yt_list.append(revised_Y)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265f2ee-8f99-4c39-a639-1c82bac8f0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ad773-9b9c-4897-9681-d643e41f36a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917093f-0c0b-4c61-b74d-d7c227eae481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9484c-7416-4e21-8045-c2f21fa9358a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7703d7-82c2-4e3b-8108-564afbeb4f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
