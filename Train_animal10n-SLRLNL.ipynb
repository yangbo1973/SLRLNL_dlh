{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fac75f-1781-44c2-94e9-e952634a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import data_loader_Animal10N as dataloader\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f974b76-a9eb-4da5-b565-a288657e66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='train batchsize') \n",
    "parser.add_argument('--lr', '--learning_rate', default=0.02, type=float, help='initial learning rate')\n",
    "parser.add_argument('--num_epochs', default=300, type=int)\n",
    "parser.add_argument('--t_w', default=10, type=int)\n",
    "parser.add_argument('--nR', default=0.04, type=float)\n",
    "parser.add_argument('--nc', default=0.2, type=float)\n",
    "parser.add_argument('--nv', default=0.8, type=float)\n",
    "parser.add_argument('--nvs', default=0.1, type=float)\n",
    "parser.add_argument('--id', default='')\n",
    "parser.add_argument('--seed', default=123)\n",
    "parser.add_argument('--gpuid', default=0, type=int)\n",
    "parser.add_argument('--data_path', default='./data/Animal10N', type=str, help='path to dataset')\n",
    "parser.add_argument('--dataset', default='Animal10N', type=str)\n",
    "args = parser.parse_args(args = ['--data_path', './data/Animal10N',\n",
    "                                 '--dataset', 'Animal10N',\n",
    "                                 '--t_w', '50',\n",
    "                                 '--batch_size','64',\n",
    "                                 '--lr','0.1',\n",
    "                                 '--num_epochs','360',\n",
    "                                 '--nR', '0.02',\n",
    "                                 '--nc','0.1',\n",
    "                                 '--nv','0.04',\n",
    "                                 '--nvs','0.02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645caeb7-89ec-45f0-860b-e3395c75bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpuid)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c17f27b-2eff-425b-a66b-867171724336",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 50000\n",
    "test_samples = 5000\n",
    "n_class = 10\n",
    "feature_num = 4096\n",
    "t_w = args.t_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decf99d1-a605-4b67-a2ef-0f64985f472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,net,):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    feature_temp = np.zeros((test_samples, feature_num))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, ind) in enumerate(test_loader):\n",
    "            ind = ind.numpy()\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            feature, output = forward_wf(net,inputs)       \n",
    "            _, predicted = torch.max(output, 1)     \n",
    "            \n",
    "            feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "                       \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).cpu().sum().item()                 \n",
    "    acc = 100.*correct/total\n",
    "    \n",
    "    test_log.write('Epoch:%d   Accuracy:%.2f\\n'%(epoch,acc))\n",
    "    test_log.flush()  \n",
    "    \n",
    "    lossb = relevant_hard_np(feature_temp)\n",
    "    return acc, lossb, feature_temp\n",
    "\n",
    "\n",
    "def linear_rampup(current, warm_up, rampup_length=16):\n",
    "    current = np.clip((current-warm_up) / rampup_length, 0.0, 1.0)\n",
    "    return args.lambda_u*float(current)\n",
    "\n",
    "\n",
    "class NegEntropy(object):\n",
    "    def __call__(self,outputs):\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        return torch.mean(torch.sum(probs.log()*probs, dim=1))\n",
    "    \n",
    "\n",
    "def create_model():\n",
    "    model = models.vgg19_bn()\n",
    "    model = model.cuda()\n",
    "    return model     \n",
    "\n",
    "def forward_wf(net, x):\n",
    "    \n",
    "    classifier = net.classifier\n",
    "    first = net.avgpool(net.features(x))\n",
    "    first = torch.reshape(first, (first.shape[0], -1))\n",
    "    feature = classifier[5](classifier[4](classifier[3](classifier[2](classifier[1](classifier[0](first))))))\n",
    "    logits = net.classifier[6](feature)[:,:n_class]\n",
    "    \n",
    "    return feature, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd574e5e-b996-4f8a-ab16-86abf8881e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_log=open('./checkpoint/SLRLNL_%s_%s'%(\n",
    "    args.dataset,str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_stats.txt','w') \n",
    "test_log=open('./checkpoint/SLRLNL_%s_%s'%(\n",
    "    args.dataset,str(datetime.date.today())+'_'+str(time.localtime().tm_hour))+'_acc.txt','w')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f167c918-9d7d-4bd4-a553-f46d4970d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Building net\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = dataloader.animal_dataloader(args.dataset,batch_size=args.batch_size,num_workers=5,root_dir=args.data_path,log=stats_log,)\n",
    "\n",
    "print('| Building net')\n",
    "net = create_model()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "opt = optim.SGD(net.parameters(),\n",
    "                lr=args.lr,\n",
    "                momentum=0.9,\n",
    "                # weight_decay=1e-5\n",
    "               )\n",
    "\n",
    "sch = optim.lr_scheduler.MultiStepLR(opt, [150, 250,], gamma = 0.1)\n",
    "\n",
    "CE = nn.CrossEntropyLoss(reduction='none')\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_ortho = Orthogonal_loss()\n",
    "\n",
    "all_loss = [[],[]] # save the history of losses from two networks\n",
    "\n",
    "traindataset, trainloader = loader.run('warmup')\n",
    "testdataset, test_loader = loader.run('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876c42d-d14f-4e1c-a060-88dd751c9723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_Y = np.array(traindataset.train_label)\n",
    "test_Y = np.array(testdataset.test_label)\n",
    "noisy_Y = np.array(traindataset.noise_label)\n",
    "revised_Y = np.array(traindataset.noise_label)\n",
    "revised_Y_before = np.array(traindataset.noise_label)\n",
    "\n",
    "Yt_list = [np.array(traindataset.noise_label)]\n",
    "acc_list = []\n",
    "loss_sep_list = [[]]\n",
    "loss_train_list = []\n",
    "Py_temp_list = []\n",
    "\n",
    "score = np.random.rand(samples,)\n",
    "score_uncertainty = np.random.rand(samples,)\n",
    "\n",
    "OOD_mask_before = np.zeros((samples,),np.bool)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "    if epoch < t_w:\n",
    "        _, trainloader = loader.run('warmup')\n",
    "    else:\n",
    "        _, trainloader = loader.run('train')\n",
    "\n",
    "    loss_train = 0\n",
    "    acc_train = 0\n",
    "    acc_train_ori = 0\n",
    "    loss_train_ori = 0\n",
    "    Py_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Pred_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    Probs_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Logits_temp = np.zeros((samples,n_class),dtype=np.float32)\n",
    "    Pred_other_temp = np.zeros((samples,),dtype=np.float32)\n",
    "    \n",
    "    feature_temp = np.zeros((samples, feature_num), dtype = np.float32)\n",
    "\n",
    "    if len(Py_temp_list) > 1:\n",
    "        score = Py_temp_list[-1]\n",
    "    else:\n",
    "        score = np.random.rand(samples,)\n",
    "        \n",
    "    OOD_mask = np.logical_and(score < np.sort(score[~OOD_mask_before])[int(len(score[~OOD_mask_before]) * args.nR)], ~OOD_mask_before)\n",
    "    \n",
    "    Y_onehot = np.eye(n_class)[revised_Y].astype(np.float32)\n",
    "    Y_onehot_0 = np.eye(n_class)[noisy_Y].astype(np.float32)\n",
    "    \n",
    "    for batch_id, (X_data, targets, ind) in enumerate(trainloader):\n",
    "        ind = ind.numpy()\n",
    "\n",
    "        Y_data = np.array(revised_Y[ind]).astype(np.int64)\n",
    "        Y_data_ori = np.array(train_Y[ind]).astype(np.int64)\n",
    "        Y_data_before = np.array(revised_Y_before[ind]).astype(np.int64)\n",
    "        temp_X = X_data.cuda()\n",
    "        opt.zero_grad()\n",
    "        Y_GPU = torch.from_numpy(Y_data).cuda()\n",
    "        Y_GPU_ori = torch.from_numpy(Y_data_ori).cuda()\n",
    "        Y_GPU_before = torch.from_numpy(Y_data_before).cuda()\n",
    "        \n",
    "        y_onehot = F.one_hot(Y_GPU.view(-1,),num_classes=n_class)\n",
    "        y_onehot_before = F.one_hot(Y_GPU_before.view(-1,),num_classes=n_class)\n",
    "        \n",
    "        feature, logits = net(temp_X)\n",
    "\n",
    "        probs = logits.softmax(1)\n",
    "        Py = torch.sum(y_onehot * probs, dim = -1)\n",
    "        Pred = probs.argmax(-1)\n",
    "        logits_other = logits - logits * y_onehot_before\n",
    "        Pred_other = torch.argmax(logits_other,dim=-1)\n",
    "        \n",
    "        if epoch < args.t_w:\n",
    "            loss = CEloss(logits,Y_GPU.view(-1,))\n",
    "        else:\n",
    "            Y_GPU = torch.where(torch.from_numpy(OOD_mask[ind]).cuda(), Pred_other, Y_GPU)\n",
    "            loss = CEloss(logits,Y_GPU.view(-1,))\n",
    "            \n",
    "        Py_temp[ind] = Py.cpu().detach().numpy()\n",
    "        Pred_temp[ind] = Pred.cpu().detach().numpy()\n",
    "\n",
    "        Probs_temp[ind] = probs.cpu().detach().numpy()\n",
    "        Logits_temp[ind] = logits.cpu().detach().numpy()\n",
    "        feature_temp[ind] = feature.cpu().detach().numpy()\n",
    "        \n",
    "        Pred_other_temp[ind] = Pred_other.cpu().detach().numpy()\n",
    "                \n",
    "        correct = (Pred == Y_GPU).sum().item()\n",
    "        correct_ori = (Pred == Y_GPU_ori).sum().item()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "        acc_train += correct\n",
    "        acc_train_ori += correct_ori\n",
    "        \n",
    "        loss_sep_list[-1].append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "    sch.step()\n",
    "    \n",
    "    loss_train/=(batch_id+1)\n",
    "    acc_train/=samples\n",
    "    acc_train_ori/=samples\n",
    "\n",
    "    print('epoch %d train complete'%epoch)\n",
    "    acc_eval, lossb_eval, feature_val = test(epoch, net)\n",
    "    \n",
    "    loss_b = relevant_hard_np(feature_temp[:10000])\n",
    "\n",
    "    acc_list.append(acc_eval)\n",
    "    Py_temp_list.append(Py_temp)\n",
    "    \n",
    "    revised_Y_before = revised_Y.copy()\n",
    "    OOD_mask_before = OOD_mask.copy()    \n",
    "        \n",
    "    if epoch < t_w:\n",
    "        select = np.zeros((samples,),dtype = np.bool)\n",
    "        score = np.random.rand(samples,)\n",
    "    else:\n",
    "        nC_points = []\n",
    "        clean_mask = np.zeros((samples,),dtype=bool)\n",
    "        Py_mean = np.zeros((samples,))\n",
    "        for j in range(len(Py_temp_list)):\n",
    "            Py_mean+=Py_temp_list[j]\n",
    "        Py_mean/=len(Py_temp_list)   \n",
    "        for j in range(n_class):\n",
    "            class_mask = noisy_Y == j\n",
    "            c_n = class_mask.sum()\n",
    "            c_th = np.sort(Py_mean[class_mask])[-int(c_n * args.nc)]\n",
    "            nC_points.append(np.where(np.logical_and(Py_mean>=c_th,class_mask))[0])\n",
    "        nC_points = np.concatenate(nC_points)\n",
    "        L_batch = 1000\n",
    "        Y_onehot = np.eye(n_class)[revised_Y]\n",
    "        zC = torch.from_numpy(feature_temp[nC_points].astype(np.float32)).cuda()\n",
    "        fC = torch.from_numpy(Probs_temp[nC_points].astype(np.float32)).cuda()\n",
    "        yC = torch.from_numpy(Y_onehot[nC_points].astype(np.float32)).cuda()\n",
    "        fCcyC = fC - yC\n",
    "        lr = 1e-6\n",
    "        learning_risk = np.zeros((samples,)) \n",
    "        for j in range(int(np.ceil(samples/L_batch))):\n",
    "            i_ind = np.arange(j*L_batch, min(samples,(j+1)*L_batch))\n",
    "            zi = torch.from_numpy(feature_temp[i_ind].astype(np.float32)).cuda()\n",
    "            fi = torch.from_numpy(Probs_temp[i_ind].astype(np.float32)).cuda()\n",
    "            yi = torch.from_numpy(Y_onehot[i_ind].astype(np.float32)).cuda()\n",
    "            zixzC = zi @ zC.transpose(1,0)\n",
    "            part_1_1 = (zixzC + 1) @ fCcyC\n",
    "            part1 = (part_1_1 * (yi-fi)).sum(dim=-1,keepdim=True)*4*lr/len(nC_points)\n",
    "            part1_all = part_1_1 * (fi-torch.ones_like(yi))*4*lr/len(nC_points)\n",
    "\n",
    "            learning_risk[i_ind] = part1.cpu().detach().numpy().ravel()\n",
    "            \n",
    "        r_ = min(args.nvs + args.nvs * (epoch - t_w), args.nv)\n",
    "        \n",
    "        th = np.sort(learning_risk)[-min(int(samples * r_),samples)]\n",
    "        select = learning_risk >= th\n",
    "        \n",
    "        revised_Y=np.where(select.ravel(), Pred_temp.ravel(), noisy_Y.ravel()).astype(int)\n",
    "\n",
    "    is_noise = revised_Y != train_Y\n",
    "    max_noised_class = -999\n",
    "    for j_ in range(n_class):\n",
    "        class_mask = train_Y == j_\n",
    "        noise_n = np.logical_and(class_mask, is_noise).sum()\n",
    "        if noise_n > max_noised_class:\n",
    "            max_noised_class = noise_n\n",
    "    \n",
    "\n",
    "    Yt_remain_noise = np.sum(is_noise)\n",
    "    end_time = time.time()\n",
    "    print_str = 'loss_b:%.4f, loss_b_eval:%.4f, train loss:%.4f, train acc:%.4f, train acc ori:%.4f,\\\n",
    "          eval acc:%.4f, time elapsed:%.4f, epoch %d train cleaned, %d samples changed,\\\n",
    "          total remain noise:%.4d, max class noise:%d'%(loss_b,\n",
    "                                                        lossb_eval,\n",
    "                                                        loss_train,\n",
    "                                                        acc_train,\n",
    "                                                        acc_train_ori, \n",
    "                                                        acc_eval,\n",
    "                                                        end_time - start_time,\n",
    "                                                        epoch,\n",
    "                                                        np.sum(revised_Y!=noisy_Y),\n",
    "                                                        Yt_remain_noise,\n",
    "                                                        max_noised_class)\n",
    "    print(print_str)\n",
    "    stats_log.write(print_str+'\\n')\n",
    "    stats_log.flush()  \n",
    "    # loss_sep_list.append([])\n",
    "    Yt_list.append(revised_Y)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad10ad7-111b-4862-9a61-1516391be0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396db517-4619-4a89-9b88-603b8bbc49d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836728e-e05b-4100-8e27-49a0c8a0a67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e92b56-1883-4b74-8209-2e9851e76e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7703d7-82c2-4e3b-8108-564afbeb4f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
